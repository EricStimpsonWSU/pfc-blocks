{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses Numba to create on-device arrays and a vector addition kernel; it is a warmup for learning how to write GPU kernels using Numba. Weâ€™ll begin with some required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is the kernel. Note that it is defined in terms of Python variables with unspecified types. When the kernel is launched, Numba will examine the types of the arguments that are passed at runtime and generate a CUDA kernel specialized for them.\n",
    "\n",
    "Note that Numba kernels do not return values and must write any output into arrays passed in as parameters (this is similar to the requirement that CUDA C/C++ kernels have `void` return type). Here we pass in `c` for the results to be written into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def f(a, b, c):\n",
    "    # like threadIdx.x + (blockIdx.x * blockDim.x)\n",
    "    tid = cuda.grid(1)\n",
    "    size = len(c)\n",
    "\n",
    "    if tid < size:\n",
    "        c[tid] = a[tid] + b[tid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cuda.to_device()` can be used create device-side copies of arrays. `cuda.device_array_like(`) creates an uninitialized array of the same shape and type as an existing array. Here we transfer two vectors and create an empty vector to hold our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "a = cuda.to_device(np.random.random(N))\n",
    "b = cuda.to_device(np.random.random(N))\n",
    "c = cuda.device_array_like(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A call to `forall()` generates an appropriate launch configuration with a 1D grid (see Kernel invocation) for a given data size and is often the simplest way of launching a kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6755152  0.39443128 0.99926269 ... 0.49445301 0.67592047 0.22192938]\n"
     ]
    }
   ],
   "source": [
    "f.forall(len(a))(a, b, c)\n",
    "print(c.copy_to_host())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also configure the grid manually using the subscripting syntax. The following example launches a grid with sufficient threads to operate on every vector element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6755152  0.39443128 0.99926269 ... 0.49445301 0.67592047 0.22192938]\n"
     ]
    }
   ],
   "source": [
    "# Enough threads per block for several warps per block\n",
    "nthreads = 256\n",
    "# Enough blocks to cover the entire vector depending on its length\n",
    "nblocks = (len(a) // nthreads) + 1\n",
    "f[nblocks, nthreads](a, b, c)\n",
    "print(c.copy_to_host())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
