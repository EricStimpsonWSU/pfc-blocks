{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summing a Vector\n",
    "Numba exposes many CUDA features, including shared memory. To demonstrate shared memory, let’s reimplement a famous CUDA solution for summing a vector which works by “folding” the data up using a successively smaller number of threads.\n",
    "\n",
    "Note that this is a fairly naive implementation, and there are more efficient ways of implementing reductions using Numba - see Monte Carlo Integration for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "from numba.types import int32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create some one dimensional data that we’ll use to demonstrate the kernel itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "a = cuda.to_device(np.arange(1024))\n",
    "nelem = len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a version of the kernel implemented using Numba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def array_sum(data):\n",
    "    tid = cuda.threadIdx.x\n",
    "    size = len(data)\n",
    "    if tid < size:\n",
    "        i = cuda.grid(1)\n",
    "\n",
    "        # Declare an array in shared memory\n",
    "        shr = cuda.shared.array(nelem, int32)\n",
    "        shr[tid] = data[i]\n",
    "\n",
    "        # Ensure writes to shared memory are visible\n",
    "        # to all threads before reducing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        s = 1\n",
    "        while s < cuda.blockDim.x:\n",
    "            if tid % (2 * s) == 0:\n",
    "                # Stride by `s` and add\n",
    "                shr[tid] += shr[tid + s]\n",
    "            s *= 2\n",
    "            cuda.syncthreads()\n",
    "\n",
    "        # After the loop, the zeroth  element contains the sum\n",
    "        if tid == 0:\n",
    "            data[tid] = shr[tid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run kernel and verify that the same result is obtained through summing data on the host as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHubWSU\\pfc-blocks\\.venv\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523776\n",
      "523776\n"
     ]
    }
   ],
   "source": [
    "array_sum[1, nelem](a)\n",
    "print(a[0])                  # 523776\n",
    "print(sum(np.arange(1024)))  # 523776"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "This algorithm can be greatly improved upon by redesigning the inner loop to use sequential memory accesses, and even further by using strategies that keep more threads active and working, since in this example most threads quickly become idle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
