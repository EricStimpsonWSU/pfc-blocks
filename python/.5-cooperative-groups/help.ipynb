{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooperative Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported features\n",
    "\n",
    "Numba’s Cooperative Groups support presently provides grid groups and grid synchronization, along with cooperative kernel launches.\n",
    "\n",
    "Cooperative groups are supported on Linux, and Windows for devices in TCC mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Grid Groups\n",
    "\n",
    "To get the current grid group, use the `cg.this_grid()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = cuda.cg.this_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synchronizing the grid is done with the `sync()` method of the grid group:\n",
    "\n",
    "```python\n",
    "g.sync()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooperative Launches\n",
    "\n",
    "Unlike the CUDA C/C++ API, a cooperative launch is invoked using the same syntax as a normal kernel launch - Numba automatically determines whether a cooperative launch is required based on whether a grid group is synchronized in the kernel.\n",
    "\n",
    "The grid size limit for a cooperative launch is more restrictive than for a normal launch - the grid must be no larger than the maximum number of active blocks on the device on which it is launched. To get maximum grid size for a cooperative launch of a kernel with a given block size and dynamic shared memory requirement, use the max_cooperative_grid_blocks() method of kernel overloads:\n",
    "\n",
    "_Kernel.max_cooperative_grid_blocks(blockdim, dynsmemsize=0)\n",
    "Calculates the maximum number of blocks that can be launched for this kernel in a cooperative grid in the current context, for the given block and dynamic shared memory sizes.\n",
    "\n",
    "Parameters\n",
    "blockdim – Block dimensions, either as a scalar for a 1D block, or a tuple for 2D or 3D blocks.\n",
    "\n",
    "dynsmemsize – Dynamic shared memory size in bytes.\n",
    "\n",
    "Returns\n",
    "The maximum number of blocks in the grid.\n",
    "\n",
    "This can be used to ensure that the kernel is launched with no more than the maximum number of blocks. Exceeding the maximum number of blocks for the cooperative launch will result in a CUDA_ERROR_COOPERATIVE_LAUNCH_TOO_LARGE error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications and Example\n",
    "\n",
    "Grid group synchronization can be used to implement a global barrier across all threads in the grid - applications of this include a global reduction to a single value, or looping over rows of a large matrix sequentially using the entire grid to operate on column elements in parallel.\n",
    "\n",
    "In the following example, rows are written sequentially by the grid. Each thread in the grid reads a value from the previous row written by it’s opposite thread. A grid sync is needed to ensure that threads in the grid don’t run ahead of threads in other blocks, or fail to see updates from their opposite thread.\n",
    "\n",
    "First we’ll define our kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import int32\n",
    "# sig = (int32[:,::1],)\n",
    "\n",
    "# @cuda.jit(sig)\n",
    "@cuda.jit\n",
    "def sequential_rows(M):\n",
    "    col = cuda.grid(1)\n",
    "    g = cuda.cg.this_grid()\n",
    "\n",
    "    rows = M.shape[0]\n",
    "    cols = M.shape[1]\n",
    "\n",
    "    for row in range(1, rows):\n",
    "        opposite = cols - col - 1\n",
    "        # Each row's elements are one greater than the previous row\n",
    "        M[row, col] = M[row - 1, opposite] + 1\n",
    "        # Wait until all threads have written their column element,\n",
    "        # and that the write is visible to all other threads\n",
    "        g.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create some empty input data and determine the grid and block sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty input data\n",
    "A = np.zeros((1024, 1024), dtype=np.int32)\n",
    "# A somewhat arbitrary choice (one warp), but generally smaller block sizes\n",
    "# allow more blocks to be launched (noting that other limitations on\n",
    "# occupancy apply such as shared memory size)\n",
    "blockdim = 32\n",
    "griddim = A.shape[1] // blockdim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we launch the kernel and print the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    0    0    0]\n",
      " [   1    1    1 ...    1    1    1]\n",
      " [   2    2    2 ...    2    2    2]\n",
      " ...\n",
      " [1021 1021 1021 ... 1021 1021 1021]\n",
      " [1022 1022 1022 ... 1022 1022 1022]\n",
      " [1023 1023 1023 ... 1023 1023 1023]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHubWSU\\pfc-blocks\\.venv\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 32 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "d:\\GitHubWSU\\pfc-blocks\\.venv\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py:888: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# Kernel launch - this is implicitly a cooperative launch\n",
    "sequential_rows[griddim, blockdim](A)\n",
    "\n",
    "# What do the results look like?\n",
    "print(A)\n",
    "#\n",
    "# [[   0    0    0 ...    0    0    0]\n",
    "#  [   1    1    1 ...    1    1    1]\n",
    "#  [   2    2    2 ...    2    2    2]\n",
    "#  ...\n",
    "#  [1021 1021 1021 ... 1021 1021 1021]\n",
    "#  [1022 1022 1022 ... 1022 1022 1022]\n",
    "#  [1023 1023 1023 ... 1023 1023 1023]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(int32, 2, 'C', False, aligned=True),)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
