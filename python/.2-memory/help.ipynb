{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transfer\n",
    "\n",
    "Even though Numba can automatically transfer NumPy arrays to the device, it can only do so conservatively by always transferring device memory back to the host when a kernel finishes. To avoid the unnecessary transfer for read-only arrays, you can use the following APIs to manually control the transfer:\n",
    "\n",
    "<style>\n",
    "p.indent {margin-left: 1em}\n",
    "</style>\n",
    "\n",
    "> `numba.cuda.device_array(shape, dtype=np.float64, strides=None, order='C', stream=0)`\n",
    "\n",
    "<p class=\"indent\">\n",
    "Allocate an empty device ndarray. Similar to numpy.empty().\n",
    "</p>\n",
    "\n",
    "> `numba.cuda.device_array_like(ary, stream=0)`\n",
    "\n",
    "<p class=\"indent\">\n",
    "Call device_array() with information from the array.\n",
    "</p>\n",
    "\n",
    "> `numba.cuda.to_device(obj, stream=0, copy=True, to=None)`\n",
    "\n",
    "<p class=\"indent\">\n",
    "Allocate and transfer a numpy ndarray or structured scalar to the device.\n",
    "</p>\n",
    "\n",
    "To copy host->device a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ary = np.arange(10)\n",
    "d_ary = cuda.to_device(ary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enqueue the transfer to a stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = cuda.stream()\n",
    "d_ary = cuda.to_device(ary, stream=stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting `d_ary` is a `DeviceNDArray`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To copy device->host:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hary = d_ary.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To copy device->host to an existing array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ary = np.empty(shape=d_ary.shape, dtype=d_ary.dtype)\n",
    "d_ary.copy_to_host(ary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enqueue the transfer to a stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hary = d_ary.copy_to_host(stream=stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the device arrays, Numba can consume any object that implements cuda array interface. These objects also can be manually converted into a Numba device array by creating a view of the GPU buffer using the following APIs:\n",
    "\n",
    "<style>\n",
    "p.indent {margin-left: 1em}\n",
    "</style>\n",
    "\n",
    "> `numba.cuda.as_cuda_array(obj, sync=True)`\n",
    "\n",
    "<p class=\"indent\">\n",
    "Create a DeviceNDArray from any object that implements the cuda array interface.\n",
    "</p>\n",
    "\n",
    "<p class=\"indent\">\n",
    "A view of the underlying GPU buffer is created. No copying of the data is done. The resulting DeviceNDArray will acquire a reference from obj.\n",
    "</p>\n",
    "\n",
    "<p class=\"indent\">\n",
    "If `sync` is `True`, then the imported stream (if present) will be synchronized.\n",
    "</p>\n",
    "\n",
    "> `numba.cuda.is_cuda_array(obj)`\n",
    "\n",
    "<p class=\"indent\">\n",
    "Test if the object has defined the *__cuda_array_interface__* attribute.\n",
    "</p>\n",
    "\n",
    "Does not verify the validity of the interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device arrays\n",
    "\n",
    "Device array references have the following methods. These methods are to be called in host code, not within CUDA-jitted functions.\n",
    "\n",
    "<style>\n",
    "p.indent {margin-left: 1em}\n",
    "p.indent2x {margin-left: 2em}\n",
    "</style>\n",
    "\n",
    "> `classnumba.cuda.cudadrv.devicearray.DeviceNDArray(shape, strides, dtype, stream=0, gpu_data=None)`\n",
    "\n",
    "<p class=\"indent\">\n",
    "An on-GPU array type\n",
    "</p>\n",
    "\n",
    "> > `copy_to_host(ary=None, stream=0)`\n",
    "\n",
    "<p class=\"indent2x\">\n",
    "Copy `self` to `ary` or create a new Numpy ndarray if `ary` is `None`.\n",
    "</p>\n",
    "\n",
    "<p class=\"indent2x\">\n",
    "If a CUDA `stream` is given, then the transfer will be made asynchronously as part as the given stream. Otherwise, the transfer is synchronous: the function returns after the copy is finished.\n",
    "</p>\n",
    "\n",
    "<p class=\"indent2x\">\n",
    "Always returns the host array.\n",
    "</p>\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHubWSU\\pfc-blocks\\.venv\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 100 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    pos = cuda.grid(1)\n",
    "    if pos < io_array.size:\n",
    "        io_array[pos] *= 2\n",
    "\n",
    "arr = np.arange(10)\n",
    "d_arr = cuda.to_device(arr)\n",
    "\n",
    "my_kernel[100, 100](d_arr)\n",
    "\n",
    "d_arr.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "p.indent {margin-left: 1em}\n",
    "p.indent2x {margin-left: 2em}\n",
    "</style>\n",
    "\n",
    "> > `is_c_contiguous()`\n",
    "\n",
    "<p class=\"indent2x\">\n",
    "Returns `True` if the array is C-contiguous.\n",
    "</p>\n",
    "\n",
    "> > `is_f_contiguous()`\n",
    "\n",
    "<p class=\"indent2x\">\n",
    "Returns `True` if the array is Fortran-contiguous.\n",
    "</p>\n",
    "\n",
    "> > `ravel(order='C',stream=0)`\n",
    "\n",
    "<p class=\"indent2x\">\n",
    "Flattens a contiguous array without changing its contents, similar to `numpy.ndarray.ravel()`.  If the array is not contiguous, raises an exception.\n",
    "</p>\n",
    "\n",
    "> > `reshape(*newshape,**kws)`\n",
    "\n",
    "<p class=\"indent2x\">\n",
    "Reshape the array without changing its contents, similar to `numpy.ndarray.reshape()`.\n",
    "</p>\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  4,  8, 12, 16],\n",
       "       [ 2,  6, 10, 14, 18]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_arr = d_arr.reshape(2, 5, order='F')\n",
    "d_arr.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note<br>\n",
    "> `DeviceNDArray` defines the cuda array interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinned memory\n",
    "\n",
    "\n",
    "<style>\n",
    "p.indent {margin-left: 1em}\n",
    "</style>\n",
    "\n",
    "> `numba.cuda.pinned(*arylist)`\n",
    "\n",
    "<p class=\"indent\">\n",
    "A context manager for temporary pinning a sequence of host ndarrays.\n",
    "</p>\n",
    "\n",
    "> `numba.cuda.pinned_array(shape, dtype=np.float64, strides=None, order='C')`\n",
    "\n",
    "<p class=\"indent\">\n",
    "Allocate an ndarray with a buffer that is pinned (pagelocked). Similar to `np.empty()`.\n",
    "</p>\n",
    "\n",
    "> `numba.cuda.pinned_array_like(ary)`\n",
    "\n",
    "<p class=\"indent\">\n",
    "Call `pinned_array()` with the information from the array.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapped memory\n",
    "\n",
    "\n",
    "<style>\n",
    "p.indent {margin-left: 1em}\n",
    "</style>\n",
    "\n",
    "> `numba.cuda.mapped(*arylist, **kws)`\n",
    "\n",
    "<p class=\"indent\">\n",
    "A context manager for temporarily mapping a sequence of host ndarrays.\n",
    "</p>\n",
    "\n",
    "> `numba.cuda.mapped_array(shape, dtype=np.float64, strides=None, order='C', stream=0, portable=False, wc=False)`\n",
    "\n",
    "<p class=\"indent\">\n",
    "Allocate a mapped ndarray with a buffer that is pinned and mapped on to the device. Similar to np.empty()\n",
    "\n",
    "<p class=\"indent\">\n",
    "Parameters\n",
    "</p>\n",
    "<p class=\"indent\">\n",
    "portable – a boolean flag to allow the allocated device memory to be usable in multiple devices.\n",
    "</p>\n",
    "\n",
    "<p class=\"indent\">\n",
    "wc – a boolean flag to enable writecombined allocation which is faster to write by the host and to read by the device, but slower to write by the host and slower to write by the device.\n",
    "</p>\n",
    "\n",
    "> `numba.cuda.mapped_array_like(ary, stream=0, portable=False, wc=False)`\n",
    "\n",
    "<p class=\"indent\">\n",
    "Call `mapped_array()` with the information from the array.\n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
